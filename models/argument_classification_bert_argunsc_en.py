# -*- coding: utf-8 -*-
"""argument_classification_BERT_ArgUNSC_en

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AhDNBQdNXZqvOjzZmUSqHcx9A8lXLihm

**Multiclass Argument Classification with Transformers**

This notebook trains a transformer-based models to perform multiclass classification on a dataset of political statements annotated as: 'claim', 'premise', and, 'non-arg'.

The goal is to automatically classify each sentence into one of these three argumentative categories using a fine-tuned Transformer model.
"""

!pip install -r requirements.txt

!pip install optuna

# Imports and Setup

import os
import random
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    Trainer, TrainingArguments, EarlyStoppingCallback, TrainerCallback
)
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.utils.class_weight import compute_class_weight
import matplotlib.pyplot as plt
import seaborn as sns
import optuna

# Disable WandB Logging
os.environ["WANDB_DISABLED"] = "true"

# Reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

# Configuration
# Model and Data Config
MODEL_NAME = "bert-base-uncased"
DATA_PATH = "ArgUNSC_en_de.xlsx"
MAX_LEN = 256
LABEL_MAP = {'claim': 0, 'premise': 1, 'non-arg': 2}
NUM_LABELS = len(LABEL_MAP)

# Utility Functions
def map_labels(dataframe):
    dataframe['Task_Label'] = dataframe['Component_Type'].map(LABEL_MAP)
    return dataframe

def split_data(df):
    train_df, temp_df = train_test_split(df, test_size=0.30, stratify=df['Task_Label'], random_state=SEED)
    val_df, test_df = train_test_split(temp_df, test_size=0.50, stratify=temp_df['Task_Label'], random_state=SEED)
    return train_df, val_df, test_df

def get_class_weights(labels):
    class_weights = compute_class_weight(class_weight="balanced", classes=np.unique(labels), y=labels)
    return torch.tensor(class_weights, dtype=torch.float)

def save_misclassified(preds, labels, dataframe, tokenizer, path):
    misclassified_mask = preds != labels
    misclassified_df = dataframe[misclassified_mask].copy()
    misclassified_df["Predicted_Label"] = preds[misclassified_mask]
    misclassified_df["True_Label"] = labels[misclassified_mask]
    misclassified_df.to_excel(path, index=False)
    print(f"Misclassified examples saved to: {path}")

def plot_confusion_matrix(y_true, y_pred, labels, title="Confusion Matrix"):
    cm = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    disp.plot(cmap='Blues')
    plt.title(title)
    plt.show()

# Dataset Class
class ArgumentDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=256):
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, index):
        row = self.dataframe.iloc[index]
        sentence = str(row['Full_Sentence']) if pd.notnull(row['Full_Sentence']) else ""
        encoding = self.tokenizer(
            sentence,
            padding='max_length',
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )
        label = int(row['Task_Label'])
        return {
            "input_ids": encoding['input_ids'].squeeze(0),
            "attention_mask": encoding['attention_mask'].squeeze(0),
            "labels": torch.tensor(label, dtype=torch.long)
        }

# Custom Trainer
class CustomTrainer(Trainer):
    def __init__(self, class_weights, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.class_weights = class_weights.to(self.model.device)
        self.loss_fn = torch.nn.CrossEntropyLoss(weight=self.class_weights)

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
      labels = inputs.pop("labels")
      outputs = model(**inputs)
      loss = self.loss_fn(outputs.logits, labels)
      return (loss, outputs) if return_outputs else loss

# Load Data
df = pd.read_excel(DATA_PATH)
df = map_labels(df)
print(f"Total dataset size: {len(df)}")

train_df, val_df, test_df = split_data(df)
print(f"Training size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}")

# Tokenizer, Dataset & Class Weights
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

train_dataset = ArgumentDataset(train_df, tokenizer, MAX_LEN)
val_dataset = ArgumentDataset(val_df, tokenizer, MAX_LEN)
test_dataset = ArgumentDataset(test_df, tokenizer, MAX_LEN)

class_weights = get_class_weights(train_df["Task_Label"].values)

# Initialize Model
def model_init():
    return AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)

# Hyperparameter Search
# Define the search space
def hp_space(trial):
    return {
        "learning_rate": trial.suggest_float("learning_rate", 1e-5, 5e-5, log=True),
        "num_train_epochs": trial.suggest_int("num_train_epochs", 2, 5),
        "per_device_train_batch_size": trial.suggest_categorical("per_device_train_batch_size", [8, 16, 32]),
    }

# TrainingArguments for the search
search_args = TrainingArguments(
    output_dir="./results_search",
    num_train_epochs=2,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    logging_dir="./logs",
    report_to="none",
    seed=42
)

# Trainer setup with model_init
trainer = CustomTrainer(
    class_weights=class_weights,
    model_init=model_init,
    args=search_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

# Run hyperparameter search
print("Starting hyperparameter search...\n")

best_run = trainer.hyperparameter_search(
    direction="minimize",
    hp_space=hp_space,
    n_trials=10,
    compute_objective=lambda metrics: metrics["eval_loss"]
)

# Log best run only
print("\nBest Hyperparameters Found:")
print(f"  - Learning rate: {best_run.hyperparameters['learning_rate']}")
print(f"  - Epochs: {best_run.hyperparameters['num_train_epochs']}")
print(f"  - Batch size: {best_run.hyperparameters['per_device_train_batch_size']}")
print(f"  - Validation loss: {best_run.objective}")

# Final Training
# Reinitialize TrainingArguments with best hyperparams
final_args = TrainingArguments(
    output_dir="./final_model",
    num_train_epochs=best_run.hyperparameters["num_train_epochs"],
    per_device_train_batch_size=best_run.hyperparameters["per_device_train_batch_size"],
    per_device_eval_batch_size=best_run.hyperparameters["per_device_train_batch_size"],
    learning_rate=best_run.hyperparameters["learning_rate"],
    logging_dir='./logs',
    report_to="none",
    seed=SEED
)

# Reinitialize the model with best config
final_model = model_init()

# Merge train + val for final training
final_train_dataset = train_dataset + val_dataset

# Recreate the trainer
final_trainer = CustomTrainer(
    class_weights=class_weights,
    model=final_model,
    args=final_args,
    train_dataset=final_train_dataset,
    eval_dataset=test_dataset
)

# Final training
final_trainer.train()

# Evaluation and Saving
print("\nEvaluating on test set...")

# Predict
predictions = final_trainer.predict(test_dataset)
y_pred = torch.argmax(torch.tensor(predictions.predictions), axis=1).numpy()
y_true = predictions.label_ids

# Report results
print(classification_report(y_true, y_pred, target_names=list(LABEL_MAP.keys())))
plot_confusion_matrix(y_true, y_pred, labels=list(LABEL_MAP.keys()))

# Clean up names for output
safe_model_name = MODEL_NAME.replace("/", "_")
safe_dataset_name = DATA_PATH.replace(".xlsx", "").replace(" ", "_")

# Save final model and tokenizer
model_path = f"./final_model_{safe_model_name}_{safe_dataset_name}"
final_trainer.save_model(model_path)
tokenizer.save_pretrained(model_path)
print(f"Final model saved to: {model_path}")

# Save misclassified examples for manual inspection
misclassified_filename = f"misclassified_{safe_model_name}_{safe_dataset_name}.xlsx"
save_misclassified(y_pred, y_true, test_df.reset_index(drop=True), tokenizer, misclassified_filename)
print(f"Misclassified examples saved to: {misclassified_filename}")

import shutil
from google.colab import files

model_path = "./final_model_bert-base-uncased_ArgUNSC_en_de"

# Zip it
shutil.make_archive(model_path, 'zip', model_path)